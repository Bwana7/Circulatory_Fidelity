\documentclass[12pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tocloft}

% ============================================================================
% PAGE LAYOUT
% ============================================================================
\geometry{margin=1in}
\onehalfspacing

% ============================================================================
% THEOREM ENVIRONMENTS
% ============================================================================
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{remark}
\newtheorem{observation}{Empirical Observation}[section]

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\CF}{\text{CF}}
\newcommand{\VFE}{F_{\text{VFE}}}
\newcommand{\RR}{F_{\text{RR}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}

% ============================================================================
% HYPERREF SETUP
% ============================================================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={Circulatory Fidelity},
    pdfauthor={Aaron Lowry}
}

% ============================================================================
% DOCUMENT
% ============================================================================
\begin{document}

% ----------------------------------------------------------------------------
% TITLE PAGE
% ----------------------------------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\LARGE\bfseries Circulatory Fidelity:\\[0.5em]
    Stability Constraints on Hierarchical Variational Inference\par}
    
    \vspace{1cm}
    
    {\large A Computational Framework Linking Structured Approximations\\
    to Dopaminergic Precision Regulation\par}
    
    \vspace{2cm}
    
    {\Large Aaron Lowry\par}
    
    \vspace{1cm}
    
    {\large December 2025\par}
    
    \vfill
    
    {\large Working Draft\par}
\end{titlepage}

% ----------------------------------------------------------------------------
% ABSTRACT
% ----------------------------------------------------------------------------
\begin{abstract}
This thesis introduces \emph{Circulatory Fidelity} (CF), a measure quantifying the statistical dependency preserved between levels of a hierarchical generative model during approximate inference. Using the Hierarchical Gaussian Filter (HGF) as a model system, we analyze the dynamical stability of variational updates under different approximation schemes.

We demonstrate that mean-field variational inference---which assumes independence between hierarchical levels---exhibits period-doubling bifurcations leading to deterministic chaos when environmental volatility exceeds a critical threshold. Structured approximations that preserve cross-level dependencies remain stable across a broader parameter range. This stability difference is characterized via Lyapunov exponent analysis and bifurcation diagrams.

Extending the analysis to three-level hierarchies, we find that mean-field instability is \emph{amplified} with depth (85-fold increase in variance), while structured approximations maintain stability. We further discover that lower hierarchical interfaces (sensory--volatility) are more critical for stability than upper interfaces (volatility--meta-volatility), suggesting that precision modulation may need to act preferentially at specific hierarchical levels.

We propose that biological inference systems may face analogous trade-offs between computational cost and dynamical stability. As a candidate neural implementation, we suggest that tonic dopamine concentration could modulate the precision of hierarchical message passing, though this mapping remains speculative. The framework generates testable predictions regarding the relationship between neuromodulatory state and belief dynamics.
\end{abstract}

\newpage
\tableofcontents
\newpage

% ----------------------------------------------------------------------------
% NOTATION
% ----------------------------------------------------------------------------
\section*{Notation}
\addcontentsline{toc}{section}{Notation}

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Symbol & Definition & Domain \\
\midrule
$z$ & Log-volatility (Level 2 state) & $\R$ \\
$x$ & Hidden state (Level 1) & $\R$ \\
$y$ & Observation & $\R$ \\
$\kappa$ & Coupling strength & $> 0$ \\
$\omega$ & Baseline log-volatility & $\R$ \\
$\vartheta$ & Volatility of volatility & $> 0$ \\
$\pi_u$ & Observation precision & $> 0$ \\
$\gamma$ & Precision weight & $> 0$ \\
$\mu_z, \mu_x$ & Posterior means & $\R$ \\
$\sigma^2_z, \sigma^2_x$ & Posterior variances & $> 0$ \\
$I(\cdot;\cdot)$ & Mutual information & $\geq 0$ \\
$H(\cdot)$ & Entropy & $\geq 0$ \\
$\lambda_{\max}$ & Maximal Lyapunov exponent & $\R$ \\
\bottomrule
\end{tabular}
\end{table}

\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}

\subsection{The Problem}

Biological agents infer hidden causes of sensory observations across multiple timescales. A canonical example: estimating both the current state of an environment and how quickly that environment is changing. These two quantities---state and volatility---interact: beliefs about volatility determine how much weight to place on new observations when updating state estimates.

The Hierarchical Gaussian Filter (HGF) formalizes this structure \citep{Mathys2011,Mathys2014}. In the HGF, a higher level encodes log-volatility, which parameterizes the expected rate of change at the lower level. Exact Bayesian inference in such models is generally intractable, motivating variational approximations.

\subsection{Variational Approximations}

Variational inference replaces the intractable true posterior $p(x,z|y)$ with a tractable approximation $q(x,z)$, chosen to minimize the Kullback-Leibler divergence from the true posterior. The most common simplification is the \emph{mean-field approximation}:
\begin{equation}
    q(x,z) = q(x)q(z)
\end{equation}

This factorization assumes independence between levels, dramatically reducing computational complexity. However, this independence assumption discards information about how states at different levels covary.

An alternative is the \emph{structured approximation}:
\begin{equation}
    q(x,z) = q(z)q(x|z)
\end{equation}
which preserves the conditional dependency of lower-level states on higher-level states. This comes at increased computational cost but retains more information about the joint posterior structure.

\subsection{This Thesis}

We investigate the dynamical consequences of these approximation choices. Our central empirical finding (from simulation) is that mean-field variational updates can become dynamically unstable---exhibiting bifurcations and chaos---under conditions where structured approximations remain stable.

We formalize this stability difference through a quantity we term \emph{Circulatory Fidelity} (CF), measuring the mutual information preserved between hierarchical levels. Specifically, CF is the normalized mutual information between hierarchical levels, using a normalization that corresponds to the \emph{uncertainty coefficient} from classical information theory \citep{Coombs1970,Theil1970}. Our contribution is not the measure itself---which has well-established precedent---but its application to analyzing the stability properties of variational approximations. We then explore the implications for understanding biological inference systems, proposing (speculatively) that neuromodulatory mechanisms may have evolved partly to maintain stable inference dynamics.

\textbf{Scope and limitations:} This is primarily a computational and theoretical analysis. The neural implementation we propose is speculative and intended to generate testable hypotheses rather than to make strong claims about biological mechanism. Throughout, we distinguish between what we can demonstrate formally, what we observe in simulation, and what we conjecture.

% ============================================================================
% SECTION 2: THEORETICAL FRAMEWORK
% ============================================================================
\section{Theoretical Framework}

\subsection{The Hierarchical Gaussian Filter}

We work with a two-level HGF defined by the following generative model:

\textbf{Level 2 (Volatility):}
\begin{equation}
    z_t \mid z_{t-1} \sim \N(z_{t-1}, \vartheta^{-1})
\end{equation}

\textbf{Level 1 (Hidden state):}
\begin{equation}
    x_t \mid x_{t-1}, z_t \sim \N(x_{t-1}, \exp(\kappa z_t + \omega))
\end{equation}

\textbf{Observations:}
\begin{equation}
    y_t \mid x_t \sim \N(x_t, \pi_u^{-1})
\end{equation}

The parameter $\vartheta$ controls how quickly volatility itself changes (sometimes called the ``hazard rate'' or ``meta-volatility''). The coupling $\kappa$ determines how strongly volatility modulates state transitions. The baseline $\omega$ sets the typical level of volatility when $z = 0$.

\subsection{Variational Updates}

Under the mean-field approximation $q(x,z) = q(x)q(z)$, the variational updates take the form of coupled fixed-point equations. At each timestep, given observation $y_t$, the posterior parameters are updated according to:

\textbf{Level 1 update:}
\begin{equation}
    \mu_x^{(t)} = \mu_x^{(t-1)} + \frac{\pi_u}{\pi_u + \hat{\pi}_x} (y_t - \mu_x^{(t-1)})
\end{equation}
where $\hat{\pi}_x = \exp(-\kappa \mu_z^{(t-1)} - \omega)$ is the expected precision at level 1.

\textbf{Level 2 update:}
\begin{equation}
    \mu_z^{(t)} = \mu_z^{(t-1)} + \frac{\kappa}{2} \frac{\hat{\pi}_x}{\vartheta + \frac{\kappa^2}{2}\hat{\pi}_x} \left( (y_t - \mu_x^{(t-1)})^2 \hat{\pi}_x - 1 \right)
\end{equation}

These update equations define a discrete dynamical system. Our analysis focuses on the stability properties of this system.

\subsubsection{Structured Variational Update}

Under the structured approximation $q(x,z) = q(z)q(x|z)$, the updates differ from mean-field by incorporating cross-level coupling. The key modification is to the Level 2 update, which gains an additional damping term:
\begin{equation}
    \mu_z^{(t)} = \mu_z^{(t-1)} + K_z \cdot \nu - \underbrace{\gamma_{zx} \cdot \Cov_q(z, x) \cdot \delta_x}_{\text{coupling term}}
\end{equation}
where $K_z$ is the standard gain, $\nu$ is the volatility prediction error, and $\gamma_{zx}$ is a coupling coefficient.

\textbf{Status of the coupling coefficient:} The specific form of $\gamma_{zx}$ used in our simulations is a \textbf{modeling approximation}, not a first-principles derivation. We use:
\begin{equation}
    \gamma_{zx} = \frac{\kappa \pi_x}{4}
\end{equation}

\textbf{Justification for this choice:}
\begin{enumerate}
    \item \textbf{Dimensional consistency:} The coefficient has units of precision, matching the update equation structure.
    \item \textbf{Correct limiting behavior:} As $\kappa \to 0$ (no coupling between levels), $\gamma_{zx} \to 0$, recovering the mean-field update.
    \item \textbf{Empirical calibration:} The factor of 1/4 was chosen to produce stable dynamics across the parameter range of interest.
\end{enumerate}

See Appendix~\ref{app:structured} for further discussion of this approximation.

\subsection{Circulatory Fidelity}

\begin{definition}[Circulatory Fidelity]
\label{def:cf}
For a joint approximate posterior $q(x,z)$ with marginal entropies $H_q(x)$ and $H_q(z)$, Circulatory Fidelity is:
\begin{equation}
    \CF = \frac{I_q(z; x)}{\min(H_q(z), H_q(x))}
\end{equation}
where $I_q(z;x) = H_q(z) + H_q(x) - H_q(z,x)$ is the mutual information between $z$ and $x$ under $q$.
\end{definition}

\textbf{Properties:}
\begin{itemize}
    \item $\CF \in [0, 1]$
    \item $\CF = 0$ if and only if $z$ and $x$ are independent under $q$
    \item $\CF = 1$ if and only if one variable is a deterministic function of the other
\end{itemize}

\begin{proposition}[CF Under Mean-Field]
\label{prop:cf-mf}
Under the mean-field approximation, $\CF = 0$.
\end{proposition}

\begin{proof}
Under $q(x,z) = q(x)q(z)$, the joint entropy decomposes: $H(x,z) = H(x) + H(z)$. Therefore $I(x;z) = H(x) + H(z) - H(x,z) = 0$. Since the numerator is zero, $\CF = 0$.
\end{proof}

\begin{corollary}
Any structured approximation with non-trivial conditional dependency has $\CF > 0$.
\end{corollary}

\begin{remark}[Choice of Normalization and Prior Work]
\label{rem:normalization}
We normalize by $\min(H(z), H(x))$ rather than the joint entropy $H(z,x)$. This normalization is not novel: it corresponds to the \emph{uncertainty coefficient} (also called the \emph{coefficient of constraint}) from classical information theory \citep{Press1967,Coombs1970}. Our contribution is not the measure itself but its application to analyzing variational approximation quality.

This normalization has two advantages:
\begin{enumerate}
    \item \textbf{Interpretability:} $\CF = 1$ when one variable is a deterministic function of the other.
    \item \textbf{Behavior in high-volatility regimes:} The joint entropy grows with marginal entropies; using it as denominator would cause CF to shrink precisely when the dependency structure matters most.
\end{enumerate}
\end{remark}

\subsection{Information Geometry}

The space of Gaussian distributions forms a Riemannian manifold with the Fisher Information Matrix (FIM) as its metric.

\begin{proposition}[FIM Block-Diagonality]
\label{prop:fim}
Under mean-field $q(x,z) = q(x)q(z)$, the Fisher Information Matrix is block-diagonal:
\begin{equation}
    \mathbf{G}_{\text{MF}} = \begin{pmatrix} G_{zz} & 0 \\ 0 & G_{xx} \end{pmatrix}
\end{equation}
Under structured $q(x,z) = q(z)q(x|z)$, the FIM generically contains non-zero off-diagonal terms:
\begin{equation}
    \mathbf{G}_{\text{struct}} = \begin{pmatrix} G_{zz} & G_{zx} \\ G_{xz} & G_{xx} \end{pmatrix}
\end{equation}
\end{proposition}

\begin{proof}
Under mean-field, $\ln q(x,z) = \ln q(x) + \ln q(z)$. Derivatives with respect to $z$-parameters depend only on $z$; derivatives with respect to $x$-parameters depend only on $x$. Cross-terms in the FIM factor as products of expectations, each of which vanishes because the score function has zero mean for exponential families:
\begin{equation}
    \E_q\left[ \frac{\partial \ln q}{\partial \theta} \right] = \frac{\partial}{\partial \theta} \int q \, d\cdot = \frac{\partial}{\partial \theta} 1 = 0
\end{equation}
Under structured approximation, the conditional $q(x|z)$ prevents this factorization. See Appendix~\ref{app:proofs} for complete proof.
\end{proof}

\textbf{Interpretation:} The off-diagonal FIM terms $G_{zx}$ encode information about how to jointly adjust beliefs at both levels in response to evidence. Their \emph{provable} absence under mean-field means this coordinated adjustment mechanism is structurally precluded by the approximation.

% ============================================================================
% SECTION 3: DYNAMICAL SYSTEMS ANALYSIS
% ============================================================================
\section{Dynamical Systems Analysis}

\subsection{The Update Map}

The mean-field variational updates define a discrete map:
\begin{equation}
    \mathbf{s}^{(t+1)} = F(\mathbf{s}^{(t)}, y_t)
\end{equation}
where $\mathbf{s} = (\mu_z, \mu_x, \sigma^2_z, \sigma^2_x)$ is the state vector. For fixed input statistics, this becomes an iterated function system whose stability we can analyze.

\subsection{Local Stability Analysis}

\begin{proposition}[Deterministic Skeleton Stability]
\label{prop:stability}
Consider the mean-field HGF $z$-dynamics under the deterministic skeleton approximation (replacing stochastic observations with their expected values). The linearized dynamics around equilibrium have Jacobian eigenvalue:
\begin{equation}
    \lambda = \frac{\vartheta}{\vartheta + \alpha}, \quad \text{where } \alpha = \frac{\kappa^2 \pi_x^*}{2}
\end{equation}
Since $\vartheta > 0$ and $\alpha > 0$, we have $0 < \lambda < 1$, implying \textbf{local stability} of the deterministic skeleton for all parameter values.
\end{proposition}

\begin{proof}
See Appendix~\ref{app:stability}.
\end{proof}

\textbf{Key insight:} This stability result for the deterministic skeleton appears to contradict the observed instabilities. The resolution is that the instability arises from \textbf{stochastic fluctuations}, not from the deterministic dynamics.

\textbf{Stochastic instability mechanism:} In the full stochastic system, large prediction errors can drive large updates. The update:
\begin{equation}
    \mu_z^{(t+1)} = \mu_z^{(t)} + K_z \cdot ((\delta^{(t)})^2 \pi_x - 1)
\end{equation}
has variance that depends on the fourth moment of $\delta$, which can exceed the mean-squared response, leading to variance growth.

\subsection{Bifurcation Structure}

\begin{observation}
\label{obs:bifurcation}
In numerical simulations with parameters $\kappa = 1$, $\omega = -2$, $\pi_u = 10$, we observe:
\begin{itemize}
    \item For $\vartheta < \vartheta_c$, the mean-field dynamics converge to a stable fixed point
    \item At $\vartheta \approx \vartheta_c$, a period-doubling bifurcation occurs
    \item For $\vartheta_c < \vartheta < \vartheta_{\text{chaos}}$, successive period-doublings are observed
    \item For $\vartheta > \vartheta_{\text{chaos}}$, the dynamics appear chaotic (positive Lyapunov exponent)
\end{itemize}
The critical values depend on parameters and noise realization:
\begin{equation}
    \vartheta_c \in [0.04, 0.06], \quad \vartheta_{\text{chaos}} \in [0.10, 0.15]
\end{equation}
\end{observation}

\begin{remark}[Resolution of the Stability Paradox]
\label{rem:paradox}
The deterministic skeleton analysis (\Cref{prop:stability}) predicts stability for all $\vartheta > 0$, yet simulations clearly show instabilities. This apparent contradiction is resolved by recognizing that \textbf{deterministic and stochastic stability are distinct phenomena}:

\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
Analysis & Object of Study & Prediction \\
\midrule
Deterministic skeleton & Dynamics of \emph{expected} state & Stable for all $\vartheta > 0$ \\
Full stochastic system & Dynamics of \emph{state distribution} & Unstable for small $\vartheta$ \\
\bottomrule
\end{tabular}
\end{center}

The instability arises not from the fixed point being unstable, but from \textbf{variance growth}: large prediction errors drive large updates, and when the gain $K_z$ is high, the variance of the state grows even though its mean remains stable.

This is analogous to noise-induced phenomena in other dynamical systems (e.g., stochastic resonance, noise-induced transitions).

\textbf{Implications:}
\begin{enumerate}
    \item Linear stability analysis is insufficient for stochastic systems
    \item The observed bifurcations are properties of the \emph{stochastic} HGF, not artifacts
    \item Proving stochastic instability requires analyzing the evolution of the full probability distribution
\end{enumerate}
\end{remark}

\subsection{Lyapunov Exponent Computation}

\begin{definition}[Maximal Lyapunov Exponent]
The maximal Lyapunov exponent characterizes the average rate of separation of nearby trajectories:
\begin{equation}
    \lambda_{\max} = \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^{T} \ln \frac{\|\delta \mathbf{s}^{(t)}\|}{\|\delta \mathbf{s}^{(t-1)}\|}
\end{equation}
where $\delta\mathbf{s}^{(t)}$ is the separation vector, periodically renormalized to prevent overflow.
\end{definition}

\begin{observation}
\label{obs:lyapunov}
For $\vartheta = 0.15$ (within the chaotic regime):

\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
Approximation & $\lambda_{\max}$ & 95\% CI \\
\midrule
Mean-field & $+0.8$ to $+1.5$ & varies with seed \\
Structured & $-0.05$ to $+0.02$ & typically $< 0$ \\
\bottomrule
\end{tabular}
\end{center}

The structured approximation yields $\lambda_{\max}$ near zero or slightly negative, indicating stable dynamics. The mean-field approximation yields positive $\lambda_{\max}$, indicating chaos.
\end{observation}

\subsection{Interpretation}

The structured approximation's stability advantage can be understood intuitively: by maintaining cross-level correlations, it provides a ``damping'' effect where beliefs at each level constrain each other. Under mean-field, the levels update independently, allowing oscillations to grow unchecked.

However, we caution against over-interpreting these results:
\begin{enumerate}
    \item Our analysis is limited to the two-level HGF; behavior in deeper hierarchies may differ
    \item The specific bifurcation thresholds are parameter-dependent
    \item Biological inference systems likely differ in important ways from the idealized models studied here
\end{enumerate}

The first concern---generalization to deeper hierarchies---we address directly in the following section.

\subsection{Extension to Three-Level Hierarchies}
\label{sec:three-level}

A significant limitation of the preceding analysis is its restriction to a two-level hierarchy. Biological inference systems operate over many levels: from sensory processing through perceptual inference to abstract reasoning. Does the CF-stability relationship hold in deeper hierarchies, or is it an artifact of our simplified model?

To address this question, we extend our analysis to the three-level HGF, which introduces a \emph{meta-volatility} level tracking how the volatility itself changes over time.

\subsubsection{Three-Level Generative Model}

The three-level HGF has the following structure \citep{Mathys2014}:

\textbf{Level 3 (Meta-volatility):}
\begin{equation}
    z_{3,t} \mid z_{3,t-1} \sim \N(z_{3,t-1}, \vartheta_3^{-1})
\end{equation}

\textbf{Level 2 (Log-volatility):}
\begin{equation}
    z_{2,t} \mid z_{2,t-1}, z_{3,t} \sim \N(z_{2,t-1}, \exp(\kappa_3 z_{3,t} + \omega_3))
\end{equation}

\textbf{Level 1 (Hidden state):}
\begin{equation}
    z_{1,t} \mid z_{1,t-1}, z_{2,t} \sim \N(z_{1,t-1}, \exp(\kappa_2 z_{2,t} + \omega_2))
\end{equation}

\textbf{Observations:}
\begin{equation}
    y_t \mid z_{1,t} \sim \N(z_{1,t}, \pi_u^{-1})
\end{equation}

This model captures environments where not only the hidden state changes (level 1), but the rate of change varies (level 2), and the stability of that rate also varies (level 3).

\subsubsection{Approximation Schemes}

With three levels, we have additional approximation options:

\textbf{Full mean-field:}
\begin{equation}
    q(z_1, z_2, z_3) = q(z_1)q(z_2)q(z_3)
\end{equation}
All levels update independently.

\textbf{Fully structured (Markov):}
\begin{equation}
    q(z_1, z_2, z_3) = q(z_3)q(z_2 \mid z_3)q(z_1 \mid z_2)
\end{equation}
Respects the generative model's conditional independence structure.

\textbf{Bottom-structured only:}
\begin{equation}
    q(z_1, z_2, z_3) = q(z_3)q(z_2)q(z_1 \mid z_2)
\end{equation}
Maintains coupling only at the 1--2 interface (sensory--volatility).

\textbf{Top-structured only:}
\begin{equation}
    q(z_1, z_2, z_3) = q(z_3)q(z_2 \mid z_3)q(z_1)
\end{equation}
Maintains coupling only at the 2--3 interface (volatility--meta-volatility).

These partial structuring variants allow us to test which hierarchical interface is most critical for stability.

\subsubsection{Pairwise Circulatory Fidelity}

For three levels, we define CF at each interface:
\begin{align}
    \CF_{12} &= \frac{I_q(z_1; z_2)}{\min(H_q(z_1), H_q(z_2))} \\
    \CF_{23} &= \frac{I_q(z_2; z_3)}{\min(H_q(z_2), H_q(z_3))}
\end{align}

Under mean-field, both $\CF_{12} = \CF_{23} = 0$. Under structured approximations, one or both may be positive depending on which interfaces maintain coupling.

\subsubsection{Simulation Results}

We simulated the three-level HGF with parameters $\kappa_2 = \kappa_3 = 1$, $\omega_2 = \omega_3 = -2$, $\pi_u = 10$, varying the meta-volatility parameter $\vartheta_3$.

\textbf{Finding 1: Mean-field instability is amplified with depth.}

\begin{center}
\begin{tabular}{@{}lcccc@{}}
\toprule
System & \multicolumn{2}{c}{Mean-Field Var($\mu_2$)} & \multicolumn{2}{c}{Structured Var($\mu_2$)} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & Value & Change & Value & Change \\
\midrule
Two-level & 0.17 & --- & 0.10 & --- \\
Three-level & 14.49 & $\mathbf{85\times}$ & 0.56 & $5.6\times$ \\
\bottomrule
\end{tabular}
\end{center}

Adding a third level increases mean-field instability by a factor of 85, while structured approximation increases only 5.6-fold. The protective effect of structured approximation becomes \emph{more} important in deeper hierarchies.

\textbf{Finding 2: The lower interface (1--2) is more critical for stability.}

\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
Approximation & Var($\mu_2$) & Relative to Structured \\
\midrule
Fully structured & 0.56 & 1.0$\times$ \\
Bottom-only (1--2 coupling) & 0.89 & 1.6$\times$ \\
Mean-field & 14.49 & 26$\times$ \\
Top-only (2--3 coupling) & 41.44 & 74$\times$ \\
\bottomrule
\end{tabular}
\end{center}

Bottom-structured approximation achieves 94\% of the full structuring benefit. Top-only structuring is \emph{worse} than mean-field, suggesting that coupling at the sensory--volatility interface is the critical bottleneck.

\textbf{Finding 3: Mean-field freezes higher levels.}

Under mean-field approximation, level 3 dynamics are effectively frozen:

\begin{center}
\begin{tabular}{@{}lccc@{}}
\toprule
Level & Mean-Field Var & Structured Var & Ratio \\
\midrule
Level 1 (state) & 1312.97 & 1312.95 & 1.0$\times$ \\
Level 2 (volatility) & 14.93 & 0.54 & 27.8$\times$ \\
Level 3 (meta-vol) & 0.0005 & 1.29 & 0.0004$\times$ \\
\bottomrule
\end{tabular}
\end{center}

Mean-field level 3 variance is essentially zero---the system has effectively collapsed to a two-level model with frozen meta-volatility. This ``freezing'' is a pathological consequence of the independence assumption: without information flow from lower levels, level 3 has no signal to update on.

\textbf{Finding 4: CF discriminates approximation schemes.}

\begin{center}
\begin{tabular}{@{}lcccc@{}}
\toprule
Scheme & $\CF_{12}$ & $\CF_{23}$ & $I_{12}$ (nats) & $I_{23}$ (nats) \\
\midrule
Mean-field & 0.00 & 0.00 & 0.00 & 0.00 \\
Structured & 0.00 & 0.37 & 0.00 & 0.41 \\
Bottom-only & 0.00 & 0.00 & 0.00 & 0.00 \\
Top-only & 0.01 & 0.00 & 0.03 & 0.04 \\
\bottomrule
\end{tabular}
\end{center}

Mean-field shows $\CF \approx 0$ at both interfaces, confirming complete decoupling. The structured approximation maintains substantial mutual information at the 2--3 interface ($I_{23} = 0.41$ nats).

\subsubsection{Interpretation}

The three-level extension provides several important insights:

\begin{enumerate}
    \item \textbf{CF thesis strengthened:} The relationship between CF and stability holds---and becomes \emph{stronger}---in deeper hierarchies. Mean-field's problems are amplified, not attenuated, with depth.
    
    \item \textbf{Interface criticality:} Not all hierarchical interfaces are equally important. The lower interface (sensory--volatility) is critical; the upper interface (volatility--meta-volatility) contributes less to stability. This asymmetry may reflect the different signal-to-noise ratios at different levels.
    
    \item \textbf{Cascade dynamics:} Mean-field approximation causes pathological ``freezing'' of higher levels, effectively truncating the hierarchy. Structured approximation maintains active dynamics throughout.
    
    \item \textbf{Depth-dependent vulnerability:} If biological inference uses approximations with low CF, deeper hierarchical processing (e.g., abstract reasoning, long-horizon planning) may be disproportionately affected.
\end{enumerate}

\textbf{Implications for neuromodulation:} If dopamine modulates precision at hierarchical interfaces, these results suggest it may need to act preferentially at lower interfaces, or coordinate across multiple interfaces to maintain stability throughout the hierarchy.

% ============================================================================
% SECTION 4: RESOURCE-RATIONAL EXTENSIONS
% ============================================================================
\section{Resource-Rational Extensions}

\subsection{Motivation}

The structured approximation provides greater stability but requires more computation---it must track the conditional distribution $q(x|z)$ rather than just marginals. This suggests a trade-off: agents with limited computational resources might prefer mean-field approximations when volatility is low but switch to structured approximations when volatility is high.

We formalize this intuition following the resource-rationality framework of \citet{Lieder2020}.

\subsection{Deriving a Cost Function}

We seek a cost function $C(q)$ that captures the computational burden of maintaining statistical dependencies between hierarchical levels. We derive this from the structure of the variational updates themselves.

\textbf{The computational asymmetry:} Under mean-field $q(x,z) = q(x)q(z)$, the update for each level depends only on its own sufficient statistics. Under structured $q(x,z) = q(z)q(x|z)$, updating $z$ requires integrating over the conditional distribution of $x$, and vice versa.

Concretely, for Gaussian posteriors:
\begin{itemize}
    \item \textbf{Mean-field:} Store and update 4 parameters $(\mu_z, \sigma^2_z, \mu_x, \sigma^2_x)$ independently
    \item \textbf{Structured:} Store and update 5+ parameters, including conditional parameters (e.g., the slope $b$ in $\E[x|z] = a + bz$), with coupled updates
\end{itemize}

\textbf{Proposal:} The computational cost of an approximation $q$ is proportional to the mutual information it maintains:
\begin{equation}
    C(q) = c_0 \cdot I_q(z; x)
\end{equation}
where $c_0 > 0$ is a constant converting nats to computational cost units.

\textbf{Justification:}

\begin{enumerate}
    \item \textbf{Operational interpretation:} $I(z;x)$ measures the statistical dependency between levels in bits (or nats). Each bit of mutual information represents a constraint that must be maintained across updates: when $z$ changes, the system must update not just $q(z)$ but also how $q(x|z)$ depends on the new $z$ value. Under mean-field, $I(z;x) = 0$, so no such cross-level bookkeeping is required.
    
    \item \textbf{Update complexity:} In message-passing implementations, maintaining $I(z;x) > 0$ requires passing messages between levels that encode conditional sufficient statistics. The information content of these messages scales with $I(z;x)$.
    
    \item \textbf{Correct limiting behavior:} 
    \begin{itemize}
        \item $C(q) = 0$ when $q$ is mean-field (since $I(z;x) = 0$ for independent variables)
        \item $C(q) > 0$ for any structured approximation with non-trivial dependency
        \item $C(q)$ increases monotonically with dependency strength
    \end{itemize}
    
    \item \textbf{Connection to description length:} From a minimum description length perspective, $I(z;x)$ is the coding cost saved by exploiting the dependency between $z$ and $x$. Conversely, maintaining this dependency (rather than discarding it) requires the system to represent and update this additional structure.
\end{enumerate}

\textbf{For Gaussian approximations}, we can compute this explicitly. If $q(z,x)$ is jointly Gaussian with correlation $\rho$, then:
\begin{equation}
    I(z;x) = -\frac{1}{2}\ln(1 - \rho^2)
\end{equation}

This increases from 0 (when $\rho = 0$, mean-field) toward infinity (as $\rho \to \pm 1$, deterministic relationship).

\subsection{Resource-Rational Free Energy}

\begin{definition}[Resource-Rational Free Energy]
\begin{equation}
    \RR = \VFE + \beta \cdot I_q(z;x)
\end{equation}
where $\VFE$ is the standard variational free energy, $I_q(z;x)$ is the mutual information under $q$, and $\beta > 0$ weights the cost-accuracy trade-off (absorbing $c_0$).
\end{definition}

The optimal approximation minimizes $\RR$, balancing:
\begin{itemize}
    \item \textbf{Accuracy} (lower $\VFE$): Structured approximations typically achieve lower free energy by better capturing the true posterior
    \item \textbf{Cost} (lower $I_q$): Mean-field approximations are cheaper by discarding cross-level information
\end{itemize}

\textbf{Note on CF vs.\ $I(z;x)$:} We use the \emph{unnormalized} mutual information $I(z;x)$ for the cost function, not the normalized Circulatory Fidelity $\CF = I(z;x)/\min(H(z),H(x))$. The unnormalized form is appropriate here because computational cost should scale with the absolute amount of dependency, not the relative amount. CF remains useful for comparing approximation quality across systems with different entropy scales.

\subsection{Relationship to Thermodynamics}

There is a suggestive connection between maintaining mutual information and thermodynamic dissipation. Landauer's principle establishes that erasing one bit of information requires at minimum $k_B T \ln(2)$ joules. By analogy, maintaining $I(z;x)$ bits of correlation might require ongoing energetic expenditure.

However, we caution against strong thermodynamic claims:
\begin{enumerate}
    \item The relevant costs in neural systems may be computational (time, memory) rather than energetic
    \item Inference updates may not constitute logically irreversible operations in Landauer's sense
    \item Empirical estimates of neural signaling costs \citep[$\sim 10^4$ ATP/bit;][]{Laughlin1998} measure information \emph{transmission}, not correlation \emph{maintenance}
\end{enumerate}

We therefore frame our cost function as \emph{resource-rational} (bounded computation) rather than thermodynamic (bounded energy), while noting the formal similarities.

\subsection{Optimal Precision}

If we model the cost of high-precision inference as:
\begin{equation}
    C(\gamma) = \gamma \ln(\gamma / \gamma_0)
\end{equation}
then the resource-rational optimal precision is:
\begin{equation}
    \gamma^* = \gamma_0 \exp\left(-1 - \frac{1}{\beta}\frac{\partial \VFE}{\partial \gamma}\right)
\end{equation}

\textbf{Testable implication:} If this trade-off operates in biological systems, we should observe bounded precision that does not increase indefinitely with prediction error magnitude.

% ============================================================================
% SECTION 5: NEURAL IMPLEMENTATION
% ============================================================================
\section{A Candidate Neural Implementation}

\subsection{Prefatory Remarks}

This section is \textbf{highly speculative}. We propose one possible mapping between the computational framework and neural mechanisms. This mapping is a hypothesis for future testing, not a claim about established neuroscience. Alternative mappings are possible, and the dopamine system's computational role remains actively debated.

\subsection{Competing Theories of Dopamine Function}

Before presenting our hypothesis, we acknowledge that dopamine's computational role is contested. Major frameworks include:

\textbf{Reward Prediction Error (RPE) theory} \citep{Schultz1997}: Phasic dopamine signals encode the difference between received and expected reward. This is the dominant interpretation of midbrain dopamine neuron firing, supported by extensive electrophysiological evidence.

\textbf{Precision/gain modulation theory} \citep{Friston2012,Schwartenbeck2015}: Dopamine modulates the precision or gain of neural computations, affecting how strongly prediction errors influence belief updates.

\textbf{Motivational salience theory}: Dopamine signals the motivational significance of stimuli, regardless of valence.

\textbf{Vigor/effort theory}: Dopamine modulates the vigor of actions and willingness to expend effort.

These theories are not mutually exclusive---dopamine likely serves multiple computational functions, possibly via distinct circuits or timescales. Our proposal aligns most closely with the precision/gain modulation view, but we emphasize that this is one interpretation among several viable alternatives.

\subsection{A Possible Dopamine-Precision Mapping}

\emph{If} dopamine modulates precision (which is not established), and \emph{if} tonic dopamine concentration is the relevant signal (rather than phasic bursts or receptor-specific effects), then we can ask: what functional form might relate dopamine concentration to precision?

We propose (tentatively) that tonic dopamine concentration \emph{could} implement a precision-like parameter $\gamma$:
\begin{itemize}
    \item Higher tonic dopamine $\to$ higher $\gamma$ $\to$ stronger weighting of prediction errors
    \item Lower tonic dopamine $\to$ lower $\gamma$ $\to$ greater reliance on prior beliefs
\end{itemize}

\subsection{One Possible Transfer Function: Hill Kinetics}

Rather than positing an arbitrary functional form, we derive \emph{one candidate} transfer function from receptor binding kinetics. This grounds the curve shape in biophysics, though the mapping from receptor occupancy to precision remains assumed.

Consider dopamine ($D$) binding to D2 receptors ($R$) with dissociation constant $K_d$:
\begin{equation}
    R + D \rightleftharpoons RD, \quad K_d = \frac{[R][D]}{[RD]}
\end{equation}

At equilibrium, the fractional receptor occupancy $\theta$ is given by the Hill equation:
\begin{equation}
    \theta = \frac{D^n}{K_d^n + D^n}
\end{equation}
where $n$ is the Hill coefficient ($n \approx 1$ for D2 receptors).

\textbf{Linking assumption (not derived):} We assume that D2 receptor occupancy modulates neural gain, and that this gain corresponds to precision:
\begin{equation}
    \gamma(D) = \gamma_{\max} \cdot \theta = \gamma_{\max} \cdot \frac{D^n}{K_d^n + D^n}
\end{equation}

\textbf{What is derived vs.\ assumed:}
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
Component & Status & Basis \\
\midrule
Hill equation form & Derived & Equilibrium thermodynamics \\
$K_d \approx 20$ nM & Constrained & Measured D2 receptor affinity \\
$n \approx 1$ & Constrained & D2 receptor biophysics \\
Occupancy $\to$ gain & \textbf{Assumed} & Plausible but unproven \\
Gain $\to$ precision & \textbf{Assumed} & Theoretical hypothesis \\
$\gamma_{\max}$ & Free parameter & Must be fit to data \\
\bottomrule
\end{tabular}
\end{center}

The chain of assumptions (occupancy $\to$ gain $\to$ precision) is the weakest link. Alternative mappings---e.g., through intracellular signaling cascades, synaptic plasticity, or network-level effects---might be more appropriate.

\subsection{Relationship to RPE Theory}

Our proposal does not contradict RPE theory but suggests a complementary role:
\begin{itemize}
    \item \emph{Phasic} dopamine (rapid bursts) $\to$ RPE signaling (well-established)
    \item \emph{Tonic} dopamine (baseline levels) $\to$ precision modulation (speculative)
\end{itemize}

This phasic/tonic distinction \emph{might} map onto receptor subtypes:
\begin{itemize}
    \item D2 receptors ($K_d \approx 10$--30 nM) have affinity in the tonic range
    \item D1 receptors ($K_d \approx 1$--10 $\mu$M) require phasic burst concentrations
\end{itemize}

However, we caution that this is an oversimplification. Both receptor types are present throughout dopamine circuits, their effects depend on cellular context, and the phasic/tonic distinction itself is debated. We offer this as one possible interpretation, not as settled neuroscience.

% ============================================================================
% SECTION 6: COMPUTATIONAL METHODS
% ============================================================================
\section{Computational Methods}

\subsection{Implementation}

We implement the HGF and variational inference using custom Julia code. The core update equations follow \citet{Mathys2014} with modifications for the structured approximation.

\textbf{Simplifications:}
\begin{enumerate}
    \item We use Gaussian approximations throughout
    \item The structured approximation uses a first-order approximation for coupling terms
    \item Lyapunov exponents are computed for the mean dynamics, ignoring posterior variance evolution
\end{enumerate}

\subsection{Lyapunov Computation}

We compute Lyapunov exponents using the Benettin algorithm:
\begin{enumerate}
    \item Initialize reference and perturbed trajectories with separation $\varepsilon = 10^{-8}$
    \item Evolve both trajectories under identical inputs
    \item Every $k$ steps ($k = 10$), measure separation, add $\log(\text{separation}/\varepsilon)$ to running sum, renormalize
    \item Average over trajectory length, discarding initial transient (1000 steps)
\end{enumerate}

\subsection{Code Availability}

Full implementation code is provided in the accompanying repository, including core model specification, Lyapunov computation, and bifurcation diagram generation.

% ============================================================================
% SECTION 7: EXPERIMENTAL TESTS
% ============================================================================
\section{Proposed Experimental Tests}

\subsection{Computational Phenotyping}

\textbf{Proposal:} Fit HGF models to behavioral data from reversal learning tasks under different pharmacological conditions.

\textbf{Prediction:} D2 antagonists $\to$ reduced CF $\to$ better fit by mean-field models.

\subsection{Dynamical Signatures}

\textbf{Proposal:} Analyze trial-by-trial learning rates for oscillatory signatures.

\textbf{Prediction:} Power spectrum of learning rate time series might show structure in conditions associated with low CF.

\textbf{Strong caveat:} Biological noise may completely obscure any bifurcation structure.

\subsection{Clinical Populations}

\textbf{Speculative predictions:}
\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
Condition & Putative DA State & CF Prediction & Behavioral Prediction \\
\midrule
Parkinson's (off med) & Low tonic DA & Low CF & Over-reliance on priors \\
Schizophrenia & Elevated striatal DA & High CF & Potentially unstable inference \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Crucial Experiment: Spectral Signatures of Period-Doubling}

The editorial challenge is: what does CF predict that alternatives do not?

\textbf{The distinguishing prediction:} CF theory predicts not merely that reduced dopamine causes noisier inference (which any theory predicts), but that it causes \textbf{qualitatively different dynamics}---specifically, quasi-periodic oscillations reflecting period-doubling bifurcations.

\textbf{Experimental design:} Within-subject pharmacological study using D2 antagonist (sulpiride 400mg) vs.\ placebo in a volatile reversal learning task.

\textbf{Primary outcome:} Power spectral density of trial-by-trial learning rates.

\textbf{CF prediction:} Under sulpiride, spectral power in the 0.02--0.10 cycles/trial band should increase, with emergent peaks at frequencies related by ratio $\approx 2$.

\begin{center}
\begin{tabular}{@{}lcc@{}}
\toprule
Prediction & CF Theory & Alternatives \\
\midrule
Increased variability & \checkmark & \checkmark \\
Oscillatory structure at specific frequencies & \checkmark & --- \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Falsification criteria:} CF would be falsified if sulpiride produces expected behavioral effects but no spectral signatures.

% ============================================================================
% SECTION 8: DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Summary of Contributions}

\begin{enumerate}
    \item \textbf{Formal definition of Circulatory Fidelity:} A normalized measure of cross-level dependency with \textbf{proven} properties ($\CF = 0$ under mean-field)
    
    \item \textbf{Information-geometric characterization:} \textbf{Proof} that the FIM is block-diagonal under mean-field
    
    \item \textbf{Stability analysis:} Derivation showing deterministic skeleton is stable; \textbf{numerical observation} that stochastic system exhibits bifurcations
    
    \item \textbf{Stochastic instability hypothesis:} Proposal that instabilities arise from variance growth
    
    \item \textbf{Resource-rational cost function:} Derivation of $C(q) = c_0 \cdot I_q(z;x)$ from the operational requirement that statistical dependencies must be maintained across updates
    
    \item \textbf{Three-level extension:} Demonstration that CF-stability relationship \textbf{strengthens} in deeper hierarchies, with 85$\times$ amplification of mean-field instability
    
    \item \textbf{Interface criticality finding:} Discovery that lower hierarchical interfaces are more critical for stability than upper interfaces
    
    \item \textbf{Neural implementation hypothesis:} Speculative but testable proposal linking CF to dopaminergic neuromodulation
    
    \item \textbf{Crucial experiment:} Protocol testing the unique prediction of CF
\end{enumerate}

\subsection{What This Work Does Not Show}

\begin{enumerate}
    \item \textbf{Not a novel information-theoretic measure:} The normalized mutual information we call ``Circulatory Fidelity'' is the established uncertainty coefficient \citep{Coombs1970}. Our contribution is its application to variational inference stability, not the measure itself.
    \item \textbf{Not a proof of biological mechanism:} The dopamine-precision mapping is one hypothesis among several viable alternatives (RPE theory, motivational salience, etc.)
    \item \textbf{Limited to Gaussian hierarchies:} While we extend to three levels, the analysis remains restricted to Gaussian generative models; non-Gaussian cases may behave differently
    \item \textbf{Not empirically validated:} The predictions remain untested
    \item \textbf{Not novel in demonstrating structured advantages:} The superiority of structured approximations is known \citep{Parr2019}
\end{enumerate}

\subsection{Relationship to Prior Work}

The advantages of structured over mean-field variational inference are well-established \citep{Parr2019,Schwobel2018}. Our contribution is to characterize this advantage in dynamical systems terms.

The resource-rationality framework \citep{Lieder2020} provides the normative foundation for our cost-accuracy trade-off.

The dopamine-precision hypothesis has been proposed by others \citep{Friston2012}. Our contribution is to connect it to the stability analysis.

\textbf{What distinguishes CF:} Previous frameworks predict structured approximations are ``better.'' CF additionally predicts \textbf{specific dynamical signatures} that should manifest when the system operates near the mean-field regime.

\subsection{Future Directions}

\textbf{Theoretical:}
\begin{itemize}
    \item Extend analysis to four or more levels (preliminary three-level results suggest continued amplification)
    \item Develop continuous-time formulation
    \item \textbf{Prove the stochastic instability mechanism}
    \item Investigate interface-specific modulation: why is the lower interface more critical?
    \item Analyze non-Gaussian generative models
\end{itemize}

\textbf{Computational:}
\begin{itemize}
    \item Systematic parameter sweeps across $\kappa$, $\omega$, $\vartheta$ space
    \item Comparison with particle filtering and MCMC
\end{itemize}

\textbf{Empirical:}
\begin{itemize}
    \item Computational phenotyping studies
    \item Pharmacological manipulations
    \item Clinical population comparisons
\end{itemize}

% ============================================================================
% SECTION 9: CONCLUSION
% ============================================================================
\section{Conclusion}

This thesis has introduced Circulatory Fidelity as a measure of cross-level dependency in hierarchical variational inference and analyzed its relationship to dynamical stability. Our main findings are:

\begin{enumerate}
    \item Mean-field variational inference in the HGF exhibits period-doubling bifurcations and chaos at high volatility
    \item Structured approximations that maintain $\CF > 0$ remain stable across a broader parameter range
    \item This stability difference can be understood geometrically through the Fisher Information Matrix structure
\end{enumerate}

We have proposed, speculatively, that biological inference systems may implement mechanisms analogous to CF maintenance, potentially through dopaminergic precision modulation. This proposal generates testable predictions that await empirical investigation.

The framework is offered as a theoretical tool for generating hypotheses, not as a completed theory of neural computation. Its value will ultimately be determined by whether the predictions it generates are borne out by experiment.

% ============================================================================
% BIBLIOGRAPHY
% ============================================================================
\bibliographystyle{apalike}
\bibliography{references}

% ============================================================================
% APPENDICES
% ============================================================================
\appendix

\section{Proof Details}
\label{app:proofs}

Complete proofs with full derivations are provided in the supplementary document. Here we summarize key results.

\subsection{Proposition 1 (FIM Block-Diagonality) --- PROVEN}

\textbf{Statement:} Under the mean-field approximation $q(x,z) = q(x)q(z)$ with Gaussian marginals, the Fisher Information Matrix is block-diagonal.

\textbf{Proof:} The FIM is defined as $G_{ij} = \E_q[\partial_i \ln q \cdot \partial_j \ln q]$.

Under mean-field: $\ln q(x,z) = \ln q(z) + \ln q(x)$

For cross-terms:
\begin{equation}
    G_{\theta_z, \theta_x} = \E_{q(z)q(x)}\left[ \frac{\partial \ln q(z)}{\partial \theta_z} \cdot \frac{\partial \ln q(x)}{\partial \theta_x} \right]
\end{equation}

By independence:
\begin{equation}
    = \E_{q(z)}\left[ \frac{\partial \ln q(z)}{\partial \theta_z} \right] \cdot \E_{q(x)}\left[ \frac{\partial \ln q(x)}{\partial \theta_x} \right]
\end{equation}

The score function has zero mean for exponential families, so all cross-terms vanish. \qed

\subsection{Proposition 2 (Local Stability) --- DERIVED}
\label{app:stability}

\textbf{Statement:} Under the deterministic skeleton, the Jacobian eigenvalue is:
\begin{equation}
    \lambda = \frac{\vartheta}{\vartheta + \alpha}
\end{equation}

\textbf{Key finding:} $0 < \lambda < 1$ for all $\vartheta > 0$, implying stability.

\textbf{Resolution:} The instability arises from \emph{stochastic} dynamics, not the deterministic skeleton.

\subsection{Proposition 3 (CF Under Mean-Field) --- PROVEN}

\textbf{Statement:} Under mean-field, $\CF = 0$.

\textbf{Proof:} Mutual information: $I(z;x) = H(z) + H(x) - H(z,x)$. Under independence: $H(z,x) = H(z) + H(x)$. Therefore $I(z;x) = 0$, and $\CF = 0$. \qed

\subsection{On Bifurcations --- NUMERICAL OBSERVATIONS}

The period-doubling bifurcations are \textbf{numerical observations}, not proven results. Observed thresholds:
\begin{itemize}
    \item First bifurcation: $\vartheta_c \in [0.04, 0.06]$
    \item Chaos onset: $\vartheta_{\text{chaos}} \in [0.10, 0.15]$
\end{itemize}

\subsection{On the Structured Update Approximation --- MODELING CHOICE}
\label{app:structured}

The coupling coefficient $\gamma_{zx} = \kappa \pi_x / 4$ is a \textbf{modeling approximation}, not a derived result.

A first-principles derivation would compute the natural gradient:
\begin{equation}
    \Delta \bm{\theta} = -\eta \mathbf{G}^{-1} \nabla_{\bm{\theta}} F
\end{equation}

The off-diagonal FIM terms would determine the exact coupling. This calculation is tractable but lengthy; we leave it for future work.

\textbf{Epistemic status:} The qualitative prediction (structured approximations are more stable) does not depend on the exact value of $\gamma_{zx}$.

\subsection{Summary of Epistemic Status}

\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
Claim & Status \\
\midrule
FIM block-diagonal under mean-field & \textbf{Proven} \\
CF $= 0$ under mean-field & \textbf{Proven} \\
CF $> 0$ under structured & \textbf{Proven} \\
Deterministic skeleton stable & \textbf{Derived} \\
Stochastic instability mechanism & \textbf{Hypothesized} \\
Period-doubling occurs & \textbf{Observed} \\
Specific bifurcation thresholds & \textbf{Observed} \\
Structured prevents bifurcation & \textbf{Observed} \\
Structured update coefficient & \textbf{Modeling approximation} \\
\bottomrule
\end{tabular}
\end{center}

\section{Simulation Parameters}

Default parameters:

\begin{center}
\begin{tabular}{@{}llll@{}}
\toprule
Parameter & Symbol & Value & Justification \\
\midrule
Coupling strength & $\kappa$ & 1.0 & Standard value \\
Baseline log-volatility & $\omega$ & $-2.0$ & Reasonable range \\
Observation precision & $\pi_u$ & 10.0 & Moderate noise \\
Volatility of volatility & $\vartheta$ & varies & Primary parameter \\
Simulation length & $T$ & 10,000 & Lyapunov convergence \\
Transient discarded & --- & 1,000 & Remove transients \\
Lyapunov renormalization & --- & every 10 steps & Standard practice \\
Random seed & --- & 42 & Reproducibility \\
\bottomrule
\end{tabular}
\end{center}

\end{document}
